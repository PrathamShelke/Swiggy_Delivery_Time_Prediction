
Steps
1)Performed Basic Data Exploration and Cleaned the Data
2)Using Data Cleaning Code in the notebook 01 created data_clean_utils.py file and created swiggy_cleaned.csv
3)Performed EDA in 02 notebook to understand data properly by performing univariate,bivariate and multivariate analysis.

4)Building Baseline Model(Linear Regression,RandomForest) in 03 notebook using 2 ways without Missing Val(NAN) and Imputation of missing vals
Evaluating both the Baseline model with metrics like MAE,R2 Score.

Through this Process of Baseline Model we found out that RandomForest is Good Baseline Model.

5)We performed below experiments with help of Dagshub(Github for ML) and MLFlow(Experiment tracking,model registry).

Performed EXP1: Where we will use RandomForest in 2 ways without Missing Values and Imputation of Missing Vals.

Performed EXP2:Where we use RandomForest with Imputation and adding Missing Indicator

Performed EXP3:Where we did model selection and found RandomForest and LightGBM as good models

Performed EXP4 and EXP5: We performed hyperparameter tuning on this 2 models to find best params through optuna.

Performed EXP6: Using RF,LightGBM as base learners we built stacking regressor and found best meta estimator model as Linear Regression.

Performed EXP7:We logged the best model params in this experiment.


6)Pushed all the Code on github by doing Code versioning.
We can do github versioning  at start of project as well.
Also created Python venv for dowloading all the required packages in requirements-dev.txt.

Performed Data versioning as well through dvc.

7)Created data_cleaning.py file in src/data folder which is basically data_clean_utils file code but tracked through dvc
Defined dvc.yaml and params.yaml file.
In dvc .yaml file added the required code for each section.
Also in params.yaml file added required hyperparameters during data preparation,training stage

Than run dvc repro to run this particular stage of dvc.yaml defined 
Then we can check the current stage using dvc dag.
Than we can push this stage into github

Similarly repeat this step 7 as we keep on adding stage in the dvc.yaml file and .py files in src folder one by one.
Until model registry stage.
Now we can run our entire dvc pipeline again when we make changes in our params.yaml file to test results on different params.

8)Added Remote(s3 bucket) on DVC pipeline.Benefit of this approach is using dvc push we can clone our entire project.
During this process we connected our dvc with s3 bucket using aws cli.

9)Created API after all this steps through Fastapi(scarlette->unicorn server,pydantic->data validation)


