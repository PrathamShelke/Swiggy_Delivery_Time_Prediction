
Steps
1)Performed Basic Data Exploration and Cleaned the Data
2)Using Data Cleaning Code in the notebook 01 created data_clean_utils.py file and created swiggy_cleaned.csv
3)Performed EDA in 02 notebook to understand data properly by performing univariate,bivariate and multivariate analysis.

4)Building Baseline Model(Linear Regression,RandomForest) in 03 notebook using 2 ways without Missing Val(NAN) and Imputation of missing vals
Evaluating both the Baseline model with metrics like MAE,R2 Score.

Through this Process of Baseline Model we found out that RandomForest is Good Baseline Model.

5)We performed below experiments with help of Dagshub(Github for ML) and MLFlow(Experiment tracking,model registry).

Performed EXP1: Where we will use RandomForest in 2 ways without Missing Values and Imputation of Missing Vals.

Performed EXP2:Where we use RandomForest with Imputation and adding Missing Indicator

Performed EXP3:Where we did model selection and found RandomForest and LightGBM as good models

Performed EXP4 and EXP5: We performed hyperparameter tuning on this 2 models to find best params through optuna.

Performed EXP6: Using RF,LightGBM as base learners we built stacking regressor and found best meta estimator model as Linear Regression.

Performed EXP7:We logged the best model params in this experiment.


6)Pushed all the Code on github by doing Code versioning.
We can do github versioning  at start of project as well.
Also created Python venv for dowloading all the required packages in requirements-dev.txt.

Performed Data versioning as well through dvc.

7)Created data_cleaning.py file in src/data folder which is basically data_clean_utils file code but tracked through dvc
Defined dvc.yaml and params.yaml file.
In dvc .yaml file added the required code for each section.
Also in params.yaml file added required hyperparameters during data preparation,training stage

Than run dvc repro to run this particular stage of dvc.yaml defined 
Then we can check the current stage using dvc dag.
Than we can push this stage into github

Similarly repeat this step 7 as we keep on adding stage in the dvc.yaml file and .py files in src folder one by one.
Until model registry stage.

Now we can run our entire dvc pipeline again when we make changes in our params.yaml file to test results on different params.
Will get different versions of model through this process of changing params.

8)Added Remote(s3 bucket) on DVC pipeline.Benefit of this approach is using dvc push we can clone our entire project.
During this process we connected our dvc with s3 bucket using aws cli.

9)Created API after all this steps through Fastapi(scarlette->unicorn server,pydantic->data validation) for model serving.
By creating app.py and data_clean_utils.py running it we can call our api and check for model prediction through swagger ui by going to /docs.
to run this api command is python app.py .

10)To test model for getting sample predictions on our data we can use sample_prediction.py file and in another terminal app.py 

11)We can check for if our model is registered on mlflow using test_model_registry.py file
by running command: python -m pytest tests/test_model_registry.py 
Also to check model performance we can run python -m pytest tests/test_model_perf.py 

12)We created github runner/actions by accessing code,python and its packages,authentication to remote where data files are stored(s3 bucket).
Than we connected github runner/actions with dagshub through authentication for automated testing as dagshub have all our model registry.

This all things written in above 2 lines we automated using CI Pipeline for automated testing.
This entire CI Pipeline is run using ci_cd.yaml file in .github/workflows folder.

For accessing the data files in s3 bucket we need to authenticate aws access keys with github actions
We can do this by going into github repository->settings->secret and variables->actions->add secret keys.

For accessing models in dagshub we need to authenticate dagshub with github actions
We can do this by first generating access token by going into our dagshub account and setting and generate token
than add that token into github secrets by using same way given above for data files

Once all the test pass we push the model to Production stage through promote_model_to_prod.py file on dagshub through same CI pipeline.

Than push all the changes to dvc if made changes in dvc pipeline by changing params.yaml file if we want new model version.
i)dvc repro
ii)dvc push 
Its done to push all data files changes to s3 bucket(i.e remote)

This entire CI Pipeline is executed automatically through github actions by pushing all the code to github again through terminal once ci_cd.yaml file is created and all the mentioned steps above are done properly.
git status
git add .
git commit -m "commit message"
git push -u origin main

If everything goes well and all tests are automated than we can see our model going into production stage on mlflow.

We can see entire working of ci pipeline after running git push command by going to github and in actions tab

13)Now we created a dockerfile named Dockerfile for containerising our entire app 
Dockerfile is nothing but the set of instructions to be performed to create docker image
We created requirements-dockers.txt for installing dependencies

To do this we first have to install docker desktop
Than to create a docker image we need to run cmd: docker build -t nameofcontainer:latest .

To check if images are created or not use cmd: docker image ls

Now after building if we want to run this docker container we have to use cmd:
 docker run --name enteranyname -p 8000:8000 -e DAGSHUB_USER_TOKEN=entertoken dockerimagename

 After running this command we can see our app is running by visiting the port mentioned http://0.0.0.0:8000/docs 
 So our app is now running perfectly in our localhost through docker container


 We can test it to see our model predictions by using this below dummy json data in required format
 {
  "ID": "0x4607",
  "Delivery_person_ID": "INDORES13DEL02",
  "Delivery_person_Age": "37",
  "Delivery_person_Ratings": "4.9",
  "Restaurant_latitude": 22.745049,
  "Restaurant_longitude": 75.892471,
  "Delivery_location_latitude": 22.765049,
  "Delivery_location_longitude": 75.912471,
  "Order_Date": "19-03-2022",
  "Time_Orderd": "11:30:00",
  "Time_Order_picked": "11:45:00",
  "Weatherconditions": "conditions Sunny",
  "Road_traffic_density": "High",
  "Vehicle_condition": 2,
  "Type_of_order": "Snack",
  "Type_of_vehicle": "motorcycle",
  "multiple_deliveries": "0",
  "Festival": "No",
  "City": "Urban"
}

14)Now we will run this image on AWS ECR(Elastic Container registry) instead of docker hub 
AWS ECR is used to store, manage, and deploy Docker container images.

To create a private repo in AWS ECR we need to search AWS ECR in aws console and create a repo with default settings

Than we will add secrets in github actions of aws ecr uri 
for eg:ECR_REPOSITORY_URI=enterreponameofecr

Also give amazon ecr full access to user by adding permissions in user 
eg:AmazonEC2ContainerRegistryFullAccess

Than push the code on github to see entire cicd pipeline running again upto aws ecr container registry.
We can see in github actions how our cicd pipeline is running.
After pipeline execution is completed check in amazon ecr if image is created 

15)Now to deploy our app on AWS will use AWS CodeDeploy 
To deploy it will follow below steps
i)Creating new iam role and selecting ec2 instance inside roles and attaching permission policies
AmazonEC2ContainerRegistryreadonly,Amazons3readonly,amazonec2forcodedeploy

Creating another i am role and selecting codeDeploy and for autoscaling 

ii)Now will go in ec2 instances and create new template give any name to instance and select guidance required
Than select os as ubuntu,instance as t2 micro as its free,generate new key pair,select the network vpc,
Go in advnaced details and under iam instance profile name select role of launchtemp created
Than scroll down and in the empty box section in last put below command as it is

#!/bin/bash

# update packages
sudo apt update -y

# install ruby required for code deploy
sudo apt install ruby-full -y

# get additional packages
sudo apt install wget -y
cd /home/ubuntu

# import the agent
wget https://aws-codedeploy-ap-south-1.s3.ap-south-1.amazonaws.com/latest/install

# install the agent
chmod +x ./install
sudo ./install auto

# run the agent
sudo systemctl start codedeploy-agent

Than click on create launch template and will see its created now in same page will see 

iii)Create autoscaling group will click on it 
now will give name to aur asg 
than in availability zones will select ap south1a,south1b
than select attach new load balancer,select application load balancer and internet facing
than will give name to load balancer,add target group name
than will turn on health check and give 300 seconds time
than will give desired capacity as 2
min and max capacity as 1 and 2 respectively
and select target tracking scaling policy
rest things defualt on page and enable monitoring in addtional settings section
will skip notification and tags section
than after reviewing entire auto scaling group will click on create auto scaling group

iv)Now will connect with our ec2 instance by entering below to commands in our terminal
chmod 400 ~/Downloads/test_delivery_time_pred.pem
 ssh -i ~/Downloads/test_delivery_time_pred.pem ubuntu@enterec2publicipv4address

than enter cmd to run codeDeploy agent : sudo systemctl status codedeploy-agent
We can see the agent is in active state of running 

v)Now will go on Aws and type codedeploy and click on create application
Than will enter name of our app and select ec2 under compute platform.

Now will click on create deployment group:give name,select service role,deployment type(inplace)
than in environment select ec2 auto scaling group and in deployment configuration select one at a time
and in load balancer select application load balancer 
Now click on create deployment group

vi)Than created appspec.yml file and added its relevant .sh file in deploy/scripts folder by adding relevant things required in it.
Than will create a new s3 bucket for deployment purpose 

vii)Now will make any small change in params.yaml and run dvc pipeline again using dvc repro
Than push all changes to github 















