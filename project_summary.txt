
Steps
1)Performed Basic Data Exploration and Cleaned the Data
2)Using Data Cleaning Code in the notebook 01 created data_clean_utils.py file and created swiggy_cleaned.csv
3)Performed EDA in 02 notebook to understand data properly by performing univariate,bivariate and multivariate analysis.

4)Building Baseline Model(Linear Regression,RandomForest) in 03 notebook using 2 ways without Missing Val(NAN) and Imputation of missing vals
Evaluating both the Baseline model with metrics like MAE,R2 Score.

Through this Process of Baseline Model we found out that RandomForest is Good Baseline Model.

5)We performed below experiments with help of Dagshub(Github for ML) and MLFlow(Experiment tracking,model registry).

Performed EXP1: Where we will use RandomForest in 2 ways without Missing Values and Imputation of Missing Vals.

Performed EXP2:Where we use RandomForest with Imputation and adding Missing Indicator

Performed EXP3:Where we did model selection and found RandomForest and LightGBM as good models

Performed EXP4 and EXP5: We performed hyperparameter tuning on this 2 models to find best params through optuna.

Performed EXP6: Using RF,LightGBM as base learners we built stacking regressor and found best meta estimator model as Linear Regression.

Performed EXP7:We logged the best model params in this experiment.


6)Pushed all the Code on github by doing Code versioning.
We can do github versioning  at start of project as well.
Also created Python venv for dowloading all the required packages in requirements-dev.txt.

Performed Data versioning as well through dvc.

7)Created data_cleaning.py file in src/data folder which is basically data_clean_utils file code but tracked through dvc
Defined dvc.yaml and params.yaml file.
In dvc .yaml file added the required code for each section.
Also in params.yaml file added required hyperparameters during data preparation,training stage

Than run dvc repro to run this particular stage of dvc.yaml defined 
Then we can check the current stage using dvc dag.
Than we can push this stage into github

Similarly repeat this step 7 as we keep on adding stage in the dvc.yaml file and .py files in src folder one by one.
Until model registry stage.

Now we can run our entire dvc pipeline again when we make changes in our params.yaml file to test results on different params.
Will get different versions of model through this process of changing params.

8)Added Remote(s3 bucket) on DVC pipeline.Benefit of this approach is using dvc push we can clone our entire project.
During this process we connected our dvc with s3 bucket using aws cli.

9)Created API after all this steps through Fastapi(scarlette->unicorn server,pydantic->data validation) for model serving.
By creating app.py and data_clean_utils.py running it we can call our api and check for model prediction through swagger ui by going to /docs.
to run this api command is python app.py .

10)To test model for getting sample predictions on our data we can use sample_prediction.py file and in another terminal app.py 

11)We can check for if our model is registered on mlflow using test_model_registry.py file
by running command: python -m pytest tests/test_model_registry.py 
Also to check model performance we can run python -m pytest tests/test_model_perf.py 

12)We created github runner/actions by accessing code,python and its packages,authentication to remote where data files are stored(s3 bucket).
Than we connected github runner/actions with dagshub through authentication for automated testing as dagshub have all our model registry.

This all things written in above 2 lines we automated using CI Pipeline for automated testing.
This entire CI Pipeline is run using ci_cd.yaml file in .github/workflows folder.

For accessing the data files in s3 bucket we need to authenticate aws access keys with github actions
We can do this by going into github repository->settings->secret and variables->actions->add secret keys.

For accessing models in dagshub we need to authenticate dagshub with github actions
We can do this by first generating access token by going into our dagshub account and setting and generate token
than add that token into github secrets by using same way given above for data files

Once all the test pass we push the model to Production stage through promote_model_to_prod.py file on dagshub through same CI pipeline.

Than push all the changes to dvc if made changes in dvc pipeline by changing params.yaml file if we want new model version.
i)dvc repro
ii)dvc push 
Its done to push all data files changes to s3 bucket(i.e remote)

This entire CI Pipeline is executed automatically through github actions by pushing all the code to github again through terminal once ci_cd.yaml file is created and all the mentioned steps above are done properly.
git status
git add .
git commit -m "commit message"
git push -u origin main



